<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
    | ECE, Virginia Tech | Fall 2025: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description"
    content="Enhancing Plant Disease Detection using Computer Vision Algorithms Project Proposal">
  <meta name="author" content="Steven An, Vikram Muruganandam, Youngjoon Park">


  <!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <style>
    body {
      padding-top: 60px;
      /* 60px to make the container go all the way to the bottom of the topbar */
    }

    .vis {
      color: #3366CC;
    }

    .data {
      color: #FF9900;
    }
  </style>

  <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
  <div class="container">
    <div class="page-header">

      <img src="diseased_plant.jpg" alt="Image of diseased plant leaf.">

      <!-- Title and Name -->
      <h1>Comparative Analysis of Advanced Deep Learning Architectures for Multi-Class Plant Disease Detection</h1>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>Steven An, Vikram Muruganandam, Youngjoon
          Park</strong></span><br>
      <span style="font-size: 18px; line-height: 1.5em;">ECE 4554/5554 Computer Vision: Course Project, Fall
        2025</span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
      <hr>
      <span style="font-size: 20px; line-height: 1.5em;">The code for this project can be found on <a
          href="https://github.com/ECE-4554-Computer-Vision-Group-Project/Code.git">GitHub, here</a>.</span><br>
      <hr>

      <!-- Goal -->
      <h3>Abstract</h3>
      <p>
        Plant diseases remain a critical threat to global food security, making early and accurate diagnosis essential
        for effective
        crop management. While traditional manual identification is labor intensive and prone to error, deep learning
        offers a scalable solution for automated detection.
        This project evaluates and compares the performance of four state-of-the-art convolutional neural network (CNN)
        architectures - ResNet50, EfficientNetV2-S, MobileNetV3,
        and YOLOv8 - in the classification of 23 healthy and diseased plant categories. Utilizing a transfer learning
        approach, we leveraged models pre-trained on ImageNet to overcome
        data scarcity and computational constraints. The study involve rigorous data preprocessing, including resizing
        and normalization specific to each architecture, alongside data
        augmentation to improve generalization. Performance was benchmarked using validation accuracy, loss metrics, and
        inference speed. the results aim to identify the optimal balance between
        computational efficiency and classification accuracy, determining which architecture is best suited for
        real-world agricultural deployment where both precision and speed are critical.

      <p></p><strong>Keywords:</strong> Plant Disease Detection, Transfer Learning, ResNet50, EfficientNetV2, YOLOv8,
      MobileNetV3, Deep Learning, Computer Vision, Multi-class Classification. <p></p>

      <br>

      <!-- Teaser Figure  -->
      <h3>Teaser figure</h3>
      <p>
      <p>
        <!-- <br><br> -->

        <!-- Main Illustrative Figure -->
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="teaser_figure.png">
        <br>
        <div style="text-align: center; font-size: 18px;">
          <strong> Fig 1.</strong> Example of Plant Disease Detection
        </div>
        <br>
        <p style="text-align: left;">As seen in the image above, the image of the apple leaf as been classified as
          diseased (scabbed).
        <p>
      </div>

      <!-- Introduction -->
      <h3>Introduction</h3>
      <p>
        Plant diseases pose one of the largest threads to global agriculture, impacting food security worldwide through
        reduced crop yields and causing enormous economic losses. Accurate detection of these
        diseases on a large scale is critical in ensuring these diseases are caught as soon as possible and before they
        can cause significant damage. Traditional methods such as manual human inspection are often time consuming and
        labor intensive,
        making it harder to implement regular checking at large scales. Using machine learning to perform these tasks is
        significantly cheaper, and does not have the same downsides as human inspection. However, ML models often lack
        accuracy and without a human
        in the loop to take accountability and due to the financial consequences of a malfunction, many users are
        hesitant to implement this technology. One method around this is to create and curating models that can achiever
        higher accuracies in plant disease detection.
      </p>

      <p>
        To do this, project builds upon existing plant disease detection systems by implementing training various models
        to perform plant disease detection using the same
        dataset. Then, we will compare the performance of these models to each other, and to previous work implementing
        a plant disease detection by <a
          href="https://www.kaggle.com/code/atharvaingle/plant-disease-classification-resnet-99-2/notebook">Atharva
          Ingle found here</a>,
        that implements plant disease classification using RESNET-9. This model was able to achieve an accuracy of
        99.2%, using a similar dataset. We aim to exceed this accuracy using one or more of the models we will be
        evaluating in this report.
      </p>

      <p>
        The main goal of the project is to compare the performance of four widely used architectures: ResNet50,
        EfficientNetV2-S,
        MobileNetV3-Small, and YOLOv8. In this project, we applied a simple and consistent computer vision preprocessing
        pipeline across all models,
        including image resizing, normalization, and light data augmentation. Keeping the input images and processing
        steps uniform allows us to fairly compare how each architecture performs under the same visual conditions and
        training setup.
      </p>

      <p>
        Throughout the project, we analyze how factors such as model size, transfer learning strategy, and training
        behavior affect
        classification accuracy. We also look at validation loss, confusion matrices, and overall stability during
        training. By doing so, we
        aim to identify which model provides the best performance.
      </p>

      <br>
      <!-- Approach -->
      <h3>Approach</h3>
      <p>
        <strong>Data Acquisition and Preparation:</strong> The project utilizes the "Plant Disease Detection" dataset
        obtained here: <a href="https://www.kaggle.com/datasets/karagwaanntreasure/plant-disease-detection"> "Plant
          Disease Detection"
          by Karagwa Ann Treasure </a>, which comprises approximately 57,000 images categorized into 23 distinct classes
        of healthy and diseased leaf samples. The dataset already includes basic offline augmentations such as
        flipping, rotation, and color jitter. Unlike the baseline approach which relied on basic offline augmentation,
        this project implements
        a dynamic data pipeline.
      <ul>
        <li>
          <strong>Preprocessing:</strong> Images are automatically resized to match the native input resolution of each
          specific architecture
          (224x224 for ResNet50/YOLOv8s-cls/MobileNetV3-S and 384x384 for EfficientNetV2-S) to preserve feature
          integrity.
        </li>
        <li>
          <strong>Normalization:</strong> Pixel values are normalized using ImageNet mean and standard deviation
          statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) to ensure compatibility with pre-trained
          weights.
        </li>
        <li>
          <strong>Split:</strong> The dataset is split into training (80%) and validation (20%) sets to monitor for
          overfitting during the learning process.
        </li>
      </ul>

      </p>
      <p>
        <strong> Comparative Architecture Strategy</strong> This project focuses on comparing the efficacy of three
        distinct deep learning architectures,
        each representing a different design philosophy:
      <ul>
        <li>
          <strong>ResNet50:</strong> Serves as the robust baseline, utilizing residual connections to solve the
          vanishing gradient problem in deep networks.
        </li>
        <li>
          <strong> EfficientNetV2-S:</strong> Represents the state-of-the-art in parameter efficieny, utilizing Fused
          MBConv layers and compound scaling to achieve high
          accuracy with fewer parameters.
        </li>
        <li>
          <strong> YOLOv8 (Classification Mode):</strong> Evaluates the trade-off between speed and accuracy, testing
          whether an architecture optimized for real-time
          object detection can perform effectively in a pure classification task.
        </li>
        <li>
          <strong> MobileNetV3-S (The Edge Specialists):</strong> Designed specifically for mobile devices, this
          architecture utilizes <strong> Neural Architecture Search (NAS)</strong>
          and <strong> Hard-Swish activation functions</strong> to minimize latency and model size while maintaining
          competitive accuracy.
        </li>
      </ul>
      </p>

      <p>
        <strong> Training and Transfer Learning</strong> To overcome the computational cost of training from scratch,
        <strong> Transfer Learning</strong>
        is applied across all models. The methodology includes:
      <ul>
        <li>
          <strong> Feature Extraction:</strong> The "backbone layers of each model are initialized with weights
          pre-trained on the ImageNet dataset and frozen.
        </li>
        <li>
          <strong> Fine-Tuning:</strong>The final fully connected classification heads are replaced to match the 23
          specific classes.
          Only these layers are optimized during the initial training phase using the Adam optimizer and Cross-Entropy
          Loss.
        </li>
        <li>
          <strong>Hardware Acceleration:</strong> Training is executed on an NVIDIA A100 GPU via Google Colab to handle
          large bath sizes and high-resolution inputs efficiently.
        </li>
      </ul>
      </p>

      <p>
        <strong> Performance Evaluation</strong> The models will be evaluated and ranked bsaeed on three key metrics:
      <ol>
        <li>
          <strong> Validation Accuracy:</strong> To measure the model's ability to generalize unseen data.
        </li>
        <li>
          <strong> Training vs. Validation Loss:</strong> To diagnose overfitting or underfitting.
        </li>
        <li>
          <strong> Inference Efficieny:</strong> To assess the computational weight of each model to determine
          suitability for
          deployment on resource-constrained agricultural devices.
        </li>
      </ol>
      </p>
      <br>

      <!-- Plans
<h3>Plans for Experimantation</h3>
<p>
  The team will begin the project by using two publicly available datasets found through Kaggle: PlantVillage 
  and New Plant Diseases Dataset. Both datasets contain labeled RGB images of healthy and diseased crop leaves
  across multiple plant species. Each dataset has approximately 87,000 images categorized into 38 different classes, 
  divided into 80% training and 20% validation sets. Both datasets were created through offline augmentation 
  from the original PlantVillage dataset, ensuring balanced class representation and good variety in lighting 
  and leaf position. A separate set of 33 test images is also provided for final evaluation.
</p>
<p>
  These datasets are well-suited for plant disease classification tasks and will serve as the foundation for 
  testing how different preprocessing techniques affect deep learning performance. The datasets already provide
  sufficient diversity in plant type, health condition, and image quality.
</p>
<p>
  The team will build on an existing implementation from Kaggle titled <a href="https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset" >“Plant Disease Classification - ResNet (99.2%)” 
  by Samir Bhattarai</a>, which uses a ResNet-9 architecture trained in PyTorch. This notebook demonstrates 
  how a convolutional neural network can achieve high accuracy on the PlantVillage dataset. We will use 
  this code as our baseline model and compare its performance to our own modified pipeline that includes
  added preprocessing methods.
</p>
<p>
  All preprocessing and experiment scripts will be written by the team using Python and OpenCV for image 
  processing. The model training and evaluation will be done using PyTorch, and experiments will run on Google 
  Colab, local system, or VT Advanced Research Center (ARC). We will record training accuracy, validation accuracy, loss curves, 
  and visualizations for analysis.
</p>

<br>
 -->
      <!--
<h4>
  Project Outline
</h4>
<ol>
  <li><strong>Experimental Setup</strong>
    <ul>
      <li>
        Datasets: PlantVillage and New Plant Diseases Dataset
      </li>
      <li>
        Model Architecture: ResNet-9
      </li>
      <li>
        Implementation Tools: Python, Google Colab/Jupyter Notebook, PyTorch, OpenCV, NumPy, Matplotlib
      </li>
      <li>
        Hardware: Google Colab GPU, local system, or VT Advanced Research Center (ARC)
      </li>
      <li>
        Evaluation Metrics: Accuracy, Precision, Recall, F1-score, and Confusion Matrix
      </li>
    </ul>
  </li>

  <br>

  <li><strong>List of Experiments</strong>
    <ul>
      <li>
        Baseline Model: Reproduce the ResNet-9 results using the original Kaggle code and dataset.
      </li>
      <li>
        Color Space Comparison: Test RGB, HSV, and LAB color-space transformations to analyze which color representation improves classification.
      </li>
      <li>
        Noise Reduction Study: Apply Gaussian and median filters to evaluate how removing background noise affects accuracy.      </li>
      <li>
        Contrast and Edge Enhancement: Use histogram equalization, Sobel/Laplacian filters, and basic texture analysis to highlight disease features.
      </li>
      <li>
        Combined Preprocessing Pipeline: Combine the best-performing preprocessing steps and compare its results against the baseline model.
      </li>
    </ul>
  </li>

  <br>

  <li><strong>Expected Outcomes</strong>
    <ul>
      <li>
        The team expects that adding preprocessing techniques will lead to improved accuracy and more 
        consistent predictions compared to the baseline ResNet-9 model trained on raw images. 
        Visualization results are expected to show that enhanced preprocessing helps the model detect and
        focus on diseased areas more accurately. However, it is uncertain which individual techniques 
        will have the greatest effect, or whether combining multiple methods will cause
        over-enhancement that reduces performance due to straying too far away from the images the model was
        trained on.
      </li>
    </ul>
  </li>

  <br>

  <li><strong>Definition of Success</strong>
    <ul>
      <li>
        The modified preprocessing pipeline achieves higher accuracy than the baseline ResNet-9 model.
      </li>
      <li>
        Visualizations show better focus on diseased leaf areas.
      </li>
      <li>
        The results clearly demonstrate that computer vision-based preprocessing can improve model 
        reliability and overall performance in plant disease detection.
      </li>
    </ul>
  </li>
</ol>

<br><br>
-->

      <!-- Main Results Figure-->
      <div style="text-align: center;">
        <img style="height: 300px;" alt="" src="table_result.png">
        <div style="text-align: center; font-size: 18px; ">
          <strong> Fig 2.</strong> Summary of Model Performance
        </div>
      </div>
      <br><br>


      <!-- Results -->
      <h3>Results</h3>


      <!-- Main Results Figure -->
      <div>
        <p>
          To evaluate the relative performance and learning stability of the selected architectures,
          we analyzed the training metrics across all four models over the course of the training duration.
          Figures (below) presents the comparative trajectories for Training Loss, Validation Loss,
          Training Accuracy, and Validation Accuracy.
        </p><br>

        <p>
          <strong> Accuracy Comparison</strong><br>
          Both YOLOv8 and EfficientNetV2-S demonstrate rapid convergence,
          crossing the 90% accuracy threshold within the first few epochs. However, YOLOv8 show in Figure 3
          ultimately separates itself from the group, reaching a peak validation
          accuracy of 99.8%. MobileNetV3 closely follows show in Figure 5, stabilizing around 98.87%, confirming its
          status
          as a high-precision architecture.
        </p>

        <p>
          ResNet50 shows a steady learning curve at 98.52% show in Figure 4, serving as a
          reliable baseline. EfficientNetV2, trails slightly lower with an accuracy of
          96% show in Figure 6. These visual gaps illustrates the expected trade-off between model lightness and
          classification
          power, although the gap is relatively small.
        </p><br>

        <p>
          <strong> Loss Convergence</strong><br>
          Across all four architectures, the Training Loss (Blue lines) and Validation Loss (Orange lines) decrease
          steadily.
          While YOLOv8 and EfficientNetV2 show the sharpest decline in loss, indicating they were able to extract
          discriminative
          features significantly faster than its peers.
        </p>
        <p>
          Crucially, none of the models exhibit significant divergence (where validation loss increases while training
          loss
          continues to decrease). This confirms that our data augmentation and transfer learning strategies successfully
          prevented overfitting, even on the larger parameter models. Therefore, these tests confirms that YOLOv8 show
          in Figure 3
          is the superior learner for this dataset, achieving the lowest final loss and highest stability, while
          EfficientNetV2-S
          still performed pretty well, it still performed the worst among the four models.
        </p>
        <div style="text-align: center; font-size: 18px; margin-top: 40px;">
          <img src="yolo_result.png" style="height: 800px;">
          <br>
          <strong> Fig 3.</strong> YOLOv8
          <br>
        </div>
      </div>
      <br><br>

      <div style="text-align: center;">
        <img style="height: 500px;" alt="" src="resnet_result.png">
      </div>
      <br>
      <div style="text-align: center; font-size: 18px;">
        <strong> Fig 4.</strong> ResNet50
      </div>
      <br>

      <div style="text-align: center;">
        <img style="height: 500px;" alt="" src="mobile_result.png">
      </div>
      <br>
      <div style="text-align: center; font-size: 18px;">
        <strong> Fig 5.</strong> MobileNetV3
      </div>
      <br>

      <div style="text-align: center;">
        <img style="height: 500px;" alt="" src="effi_result.png">
      </div>
      <br>
      <div style="text-align: center; font-size: 18px;">
        <strong> Fig 6.</strong> EfficientNetV2
      </div>
      <br>

      <div style="text-align: left; font-size: 25px; margin-top: 20px;">
        These confusion matrices illustrate how well the models classified each plant disease class.
        <br>
        <div style="text-align: center;"">
          <img src=" yolo_confusion.png" style="height: 600px;">
          <br>
          <div style="text-align: center; font-size: 18px; margin-top: 20px;">
            <strong> Fig 7.</strong> YOLOv8-Confusion Matrix
          </div>
        </div>
        <br>
      </div>
      <br><br>

      <br><br>

      <br>
      <div style="text-align: center;">
        <img style="height: 800px;" alt="" src="resnet_confusion.png">
      </div>
      <br>
      <div style="text-align: center; font-size: 18px;">
        <strong> Fig 8.</strong> ResNet50-Confusion Matrix
      </div>
      <br>

      <br><br>
      <br>
      <div style="text-align: center;">
        <img style="height: 800px;" alt="" src="mobile_confusion.png">
      </div>
      <br>
      <div style="text-align: center; font-size: 18px;">
        <strong> Fig 9.</strong> MobileNetV3-Confusion Matrix
      </div>
      <br>

      <br><br>
      <br>
      <div style="text-align: center;">
        <img style="height: 800px;" alt="" src="effi_confusion.png">
      </div>
      <br>
      <div style="text-align: center; font-size: 18px;">
        <strong> Fig 10.</strong> EfficientNetV2-Confusion Matrix
      </div>
      <br>


      <!-- Qualitative Results  -->
      <h3>Qualitative Results</h3>
      <p>
        The following figures show results of the YOLOv8 model from the validation dataset. Each leaf image is shown
        with its corresponding output and its confidence, indicating whether the leaf is diseased or not.
        The results of inputting both diseased and non-diseased leaves can be seen in the first two images. The first
        image of a tomato leaf with bacterial spots, which the model has identified with a confidence of 1.0.
        Next to it is an image of a healthy maize leaf, which the model has also identified with a confidence of 1.0.
        This reflects the high accuracy and confidence in the quantitative results seen above for the YOLOv8 model.
        However, YOLOv8 is not perfect and predicted wrong on several classes shown in Figure 12. For example, on the first image,
        the model predicted Corn Cercospora Leaf Spot with a confidence of 1.0, however the ground truth label showed Northern Leaf Blight. 
        Other misclassifications can be seen in the other images on Figure 12. This goes to show that our model may not be 
        perfect, and there is room for improvement in the future.
      </p>
      <div style="text-align: center;">
        <img style="height: 550px;" alt="" src="val_batch0_pred.jpg">
        <img style="height: 550px;" alt="" src="val_batch1_pred.jpg">
        <img style="height: 550px;" alt="" src="val_batch2_pred.jpg">
      </div>
        <br>
        <div style="text-align: center; font-size: 18px;">
          <strong> Fig 11.</strong> Validation batch predictions generated by YOLOv8.
          <div style="margin-top: 20px;">
            The model displays high-confidence correct predictions
            across diverse plant species and disease types, reflecting the 99.8% accuracy rate.
          </div>
          <br>
        </div>
        <br>
        <div style="text-align: center; font-size: 18px;">
          <img style="height: 550px;" alt="" src="fail.png">
          <br>
          <strong> Fig 12.</strong> Example of a misclassification.
        </div>
        <br>


        <!-- Conclusion  -->
        <h3>Conclusion</h3>
        <p>
          This report has described a comparative analysis of four distinct deep learning architectures—ResNet50,
          EfficientNetV2-S, MobileNetV3, and YOLOv8—for the automated
          classification of 23 plant disease categories. By utilizing a transfer learning methodology and a dynamic data
          pipeline involving specific resizing and normalization
          techniques, we successfully adapted models pre-trained on ImageNet to the domain of agricultural pathology.
        </p>
        <p>
          The experimental results offered a significant deviation from the traditional trade-off between computational
          speed and classification accuracy.
          While EfficientNetV2-S provided high accuracy (~98%) as expected from a complex architecture, and MobileNetV3
          offered extremely low latency suitable for
          edge devices, YOLOv8 emerged as the superior architecture in every metric. Achieving a remarkable
          classification accuracy of 99.8% while maintaining the fastest
          inference speed (6.4ms per image), YOLOv8 demonstrated that modern real-time object detection backbones are
          exceptionally capable feature extractors for multi-class
          classification tasks. This finding suggests that for real-world agricultural applications, where immediate
          diagnostic feedback is crucial, the YOLOv8 architecture
          offers an optimal solution without compromising on precision.
        </p>

        <p>
          <strong>Future Work</strong> <br>
          To transition this system from a research prototype to a robust field-deployable product, several key
          enhancements are proposed. Future iterations will prioritize
          incorporating "in-the-wild" datasets featuring complex backgrounds, shadows, and varying lighting conditions
          to ensure the model generalizes well beyond controlled
          laboratory images. Additionally, leveraging the high efficiency of the YOLOv8 and MobileNetV3 architectures,
          we aim to deploy the trained weights onto mobile
          platforms using frameworks like TensorFlow Lite or ONNX Runtime, enabling offline diagnostics for farmers in
          remote areas. To further refine classification performance,
          a hierarchical approach could be implemented to first identify plant species before diagnosing specific
          pathologies, thereby reducing false positives between
          similar-looking diseases on different crops. Finally, integrating explainability tools such as Grad-CAM will
          be essential for building trust with agricultural experts,
          providing visual confirmation that the model is detecting distinct disease pathology rather than relying on
          background noise or artifacts.
        </p>


        <br><br>

        <!-- References -->
        <h3>References</h3>

        <h4>Related Work</h4>
        <ul>
          <li>
            Atharva Ingle, “Plant Disease Classification - ResNet 99.2%,”
            <a href="https://www.kaggle.com/code/atharvaingle/plant-disease-classification-resnet-99-2/notebook"
              target="_blank">
              Kaggle Notebook
            </a>.
          </li>
        </ul>

        <h4>Dataset</h4>
        <ul>
          <li>
            Karagwa Ann Treasure, “Plant Disease Detection Dataset,”
            <a href="https://www.kaggle.com/datasets/karagwaanntreasure/plant-disease-detection/data" target="_blank">
              Kaggle Dataset
            </a>.
          </li>
        </ul>

        <h4>Model Architectures</h4>
        <ul>
          <li>
            Howard, A. et al., “Searching for MobileNetV3,” 2019.
            <a href="https://arxiv.org/abs/1905.02244" target="_blank">https://arxiv.org/abs/1905.02244</a>
          </li>

          <li>
            Tan, M., and Le, Q. V., “EfficientNetV2: Smaller Models and Faster Training,” 2021.
            <a href="https://arxiv.org/abs/2104.00298" target="_blank">https://arxiv.org/abs/2104.00298</a>
          </li>

          <li>
            Yaseen, M., “What is YOLOv8: An In-Depth Exploration of the Internal Features of the
            Next-Generation
            Object
            Detector,” 2024.
            <a href="https://arxiv.org/abs/2408.15857" target="_blank">https://arxiv.org/abs/2408.15857</a>
          </li>

          <li>
            He, K., Zhang, X., Ren, S., and Sun, J., “Deep Residual Learning for Image Recognition,” 2015.
            <a href="https://arxiv.org/abs/1512.03385" target="_blank">https://arxiv.org/abs/1512.03385</a>
          </li>
        </ul>
        <br><br>

        <hr>
        <footer>
          <p style="font-size: 15px;">To see this project's proposal, click <a href="project_proposal.html">here</a>
          </p>
          <p>©2025 Steven An, Vikram Muruganandam, Youngjoon Park</p>
        </footer>
      </div>
      <br><br>

      <!-- Conclusion  -->
      <h3>Conclusion</h3>
      <p>
        This report has described a comparative analysis of four distinct deep learning architectures—ResNet50,
        EfficientNetV2-S, MobileNetV3, and YOLOv8—for the automated
        classification of 23 plant disease categories. By utilizing a transfer learning methodology and a dynamic data
        pipeline involving specific resizing and normalization
        techniques, we successfully adapted models pre-trained on ImageNet to the domain of agricultural pathology.
      </p>
      <p>
        The experimental results offered a significant deviation from the traditional trade-off between computational
        speed and classification accuracy.
        While EfficientNetV2-S provided high accuracy (~98%) as expected from a complex architecture, and MobileNetV3
        offered extremely low latency suitable for
        edge devices, YOLOv8 emerged as the superior architecture in every metric. Achieving a remarkable
        classification accuracy of 99.8% while maintaining the fastest
        inference speed (6.4ms per image), YOLOv8 demonstrated that modern real-time object detection backbones are
        exceptionally capable feature extractors for multi-class
        classification tasks. This finding suggests that for real-world agricultural applications, where immediate
        diagnostic feedback is crucial, the YOLOv8 architecture
        offers an optimal solution without compromising on precision.
      </p>

      <p>
        <strong>Future Work</strong> <br>
        To transition this system from a research prototype to a robust field-deployable product, several key
        enhancements are proposed. Future iterations will prioritize
        incorporating "in-the-wild" datasets featuring complex backgrounds, shadows, and varying lighting conditions
        to ensure the model generalizes well beyond controlled
        laboratory images. Additionally, leveraging the high efficiency of the YOLOv8 and MobileNetV3 architectures,
        we aim to deploy the trained weights onto mobile
        platforms using frameworks like TensorFlow Lite or ONNX Runtime, enabling offline diagnostics for farmers in
        remote areas. To further refine classification performance,
        a hierarchical approach could be implemented to first identify plant species before diagnosing specific
        pathologies, thereby reducing false positives between
        similar-looking diseases on different crops. Finally, integrating explainability tools such as Grad-CAM will
        be essential for building trust with agricultural experts,
        providing visual confirmation that the model is detecting distinct disease pathology rather than relying on
        background noise or artifacts.
      </p>


      <br><br>

      <!-- References -->
      <h3>References</h3>

      <h4>Related Work</h4>
      <ul>
        <li>
          Atharva Ingle, “Plant Disease Classification - ResNet 99.2%,”
          <a href="https://www.kaggle.com/code/atharvaingle/plant-disease-classification-resnet-99-2/notebook"
            target="_blank">
            Kaggle Notebook
          </a>.
        </li>
      </ul>

      <h4>Dataset</h4>
      <ul>
        <li>
          Karagwa Ann Treasure, “Plant Disease Detection Dataset,”
          <a href="https://www.kaggle.com/datasets/karagwaanntreasure/plant-disease-detection/data" target="_blank">
            Kaggle Dataset
          </a>.
        </li>
      </ul>

      <h4>Model Architectures</h4>
      <ul>
        <li>
          Howard, A. et al., “Searching for MobileNetV3,” 2019.
          <a href="https://arxiv.org/abs/1905.02244" target="_blank">https://arxiv.org/abs/1905.02244</a>
        </li>

        <li>
          Tan, M., and Le, Q. V., “EfficientNetV2: Smaller Models and Faster Training,” 2021.
          <a href="https://arxiv.org/abs/2104.00298" target="_blank">https://arxiv.org/abs/2104.00298</a>
        </li>

        <li>
          Yaseen, M., “What is YOLOv8: An In-Depth Exploration of the Internal Features of the
          Next-Generation
          Object
          Detector,” 2024.
          <a href="https://arxiv.org/abs/2408.15857" target="_blank">https://arxiv.org/abs/2408.15857</a>
        </li>

        <li>
          He, K., Zhang, X., Ren, S., and Sun, J., “Deep Residual Learning for Image Recognition,” 2015.
          <a href="https://arxiv.org/abs/1512.03385" target="_blank">https://arxiv.org/abs/1512.03385</a>
        </li>
      </ul>
      <br><br>

      <hr>
      <footer>
        <p style="font-size: 15px;">To see this project's proposal, click <a href="project_proposal.html">here</a>
        </p>
        <p>©2025 Steven An, Vikram Muruganandam, Youngjoon Park</p>
      </footer>
    </div>
  </div>

  <br><br>

</body>

</html>